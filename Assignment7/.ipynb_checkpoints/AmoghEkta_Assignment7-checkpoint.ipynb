{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da195e35",
   "metadata": {},
   "source": [
    "# DS 862 - ASSIGNMENT 5\n",
    "## AMOGH RANGANATHAIAH (aranganathaiah@sfsu.edu)\n",
    "## EKTA SINGH (esingh@sfsu.edu)\n",
    "\n",
    "For this assignment, we will be using [yelp dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences), that contains Yelp reviews and the labeled sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd2fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d75174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I learned that if an electric slicer is used t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But they don't clean the chiles?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>The third one was too.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>This is the very sucks place i ever seen.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>I recommend the Hot Bagels and Deli to my frie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>Great casual (almost divey) atmosphere.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>How fortunate we are that Stefano Fabbri, owne...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3729 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0                              Wow... Loved this place.        1.0\n",
       "1     I learned that if an electric slicer is used t...        NaN\n",
       "2                      But they don't clean the chiles?        NaN\n",
       "3                                    Crust is not good.        0.0\n",
       "4             Not tasty and the texture was just nasty.        0.0\n",
       "...                                                 ...        ...\n",
       "3724                             The third one was too.        NaN\n",
       "3725          This is the very sucks place i ever seen.        NaN\n",
       "3726  I recommend the Hot Bagels and Deli to my frie...        NaN\n",
       "3727            Great casual (almost divey) atmosphere.        NaN\n",
       "3728  How fortunate we are that Stefano Fabbri, owne...        NaN\n",
       "\n",
       "[3729 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "yelp_data = pd.read_csv('yelp_labelled.txt', sep = \"\\t\", names =['text','sentiment'])\n",
    "yelp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1432ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "yelp_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8869a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Separate the data into features and target variable\n",
    "# X = yelp_data['text']\n",
    "# y = yelp_data['sentiment']\n",
    "\n",
    "# # Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23fe43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download stopwords from nltk if not already done\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from a given text\n",
    "def remove_stopwords(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize and convert to lowercase\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to remove stop words from the text data\n",
    "yelp_data['text'] = yelp_data['text'].apply(remove_stopwords)\n",
    "\n",
    "# Now separate the data into features and target variable\n",
    "X = yelp_data['text']\n",
    "y = yelp_data['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a3739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 10.0}\n",
      "Accuracy on Test Set: 0.755\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.78      0.75        96\n",
      "         1.0       0.78      0.73      0.76       104\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.76      0.76      0.75       200\n",
      "weighted avg       0.76      0.76      0.76       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature Extraction using Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Build Naive Bayes Classifier\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0]}  # 'alpha' is the smoothing parameter\n",
    "\n",
    "# Use GridSearchCV to find the best 'alpha'\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_naive_bayes = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "y_pred = best_naive_bayes.predict(X_test_bow)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370ef2b",
   "metadata": {},
   "source": [
    "### Observation for Bag-of-Words for Feature Extraction\n",
    "\n",
    "1. The optimal smoothing parameter alpha for the Multinomial Naive Bayes model was found to be 10.\n",
    "2. The model achieved an overall accuracy of 75.5% on the test data.\n",
    "3. Class 0 (Negative Sentiment):\n",
    "        a. Precision (0.73): Of all reviews predicted as negative, 73% were actually negative. The model makes some false positive errors when identifying negative reviews.\n",
    "    b. Recall (0.78): Of all actual negative reviews, the model correctly identified 78%. This indicates that the model performs well in capturing most negative reviews.\n",
    "    c. F1-Score (0.75): The harmonic mean of precision and recall, indicating a good balance between both metrics.\n",
    "4. Class 1 (Positive Sentiment): </br>\n",
    "    a. Precision (0.78): Of all reviews predicted as positive, 78% were correctly identified as positive. This shows that the model is more reliable in predicting positive sentiment compared to negative.</br>\n",
    "    b. Recall (0.73): Of all actual positive reviews, 73% were correctly identified. This suggests some missed positive reviews (false negatives).</br>\n",
    "    c. F1-Score (0.76): Indicates a slightly lower balance between precision and recall compared to the negative class.</br>\n",
    "5. The Multinomial Naive Bayes model performs well in identifying both positive and negative sentiments, with slightly better precision for positive sentiment but higher recall for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffb9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 20.0}\n",
      "Accuracy on Test Set: 0.79\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.84      0.79        96\n",
      "         1.0       0.84      0.74      0.79       104\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.80      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Feature Extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 2: Build Naive Bayes Classifier\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Step 3: Hyperparameter Tuning using GridSearchCV\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 20.0, 30.0, 40.0, 50.0]}  # 'alpha' is the smoothing parameter\n",
    "\n",
    "# Use GridSearchCV to find the best 'alpha'\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_naive_bayes = grid_search.best_estimator_\n",
    "\n",
    "# Step 4: Evaluate the performance on the test set\n",
    "y_pred = best_naive_bayes.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8d76a",
   "metadata": {},
   "source": [
    "### Observation for TF-IDF for Feature Extraction, with MultinomialNB\n",
    "1. The optimal value of alpha was determined to be 20.\n",
    "2. The model achieved an overall accuracy of 79%.\n",
    "3. Class 0 (Negative Sentiment): </br>\n",
    "    a. Precision (0.75): The model correctly identified 75% of the reviews it predicted as negative. It still has some false positives when labeling negative reviews. </br>\n",
    "    b. Recall (0.84): The model successfully identified 84% of all actual negative reviews. This is high, indicating that most negative reviews were detected. </br>\n",
    "    c. F1-Score (0.79): Balances precision and recall, showing good overall performance for the negative class.\n",
    "4. Class 1 (Positive Sentiment): </br>\n",
    "    a. Precision (0.84): The model correctly identified 84% of the reviews it predicted as positive, which is good precision. </br>\n",
    "    b. Recall (0.74): The model successfully identified 74%% of all actual positive reviews, meaning it is better at capturing positive sentiment compared to negative. </br>\n",
    "    c. F1-Score (0.79): Shows a reasonably good balance between precision and recall for positive sentiment.\n",
    "5. The TF-IDF-based Multinomial Naive Bayes model achieves a balanced performance, similar to the Bag-of-Words approach. The model exhibits strong precision for positive sentiment and high recall for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb140021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'var_smoothing': 0.01}\n",
      "Accuracy on Test Set: 0.71\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.62      0.67        96\n",
      "         1.0       0.69      0.79      0.74       104\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Feature Extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()  # Convert to array for GaussianNB\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Step 2: Build Gaussian Naive Bayes Classifier\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Step 3: Hyperparameter Tuning using GridSearchCV\n",
    "# GaussianNB has limited hyperparameters, mainly 'var_smoothing'\n",
    "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]}\n",
    "\n",
    "# Use GridSearchCV to find the best 'var_smoothing'\n",
    "grid_search = GridSearchCV(gaussian_nb, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_gaussian_nb = grid_search.best_estimator_\n",
    "\n",
    "# Step 4: Evaluate the performance on the test set\n",
    "y_pred = best_gaussian_nb.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb5e61",
   "metadata": {},
   "source": [
    "### Observation for TF-IDF for Feature Extraction, with GaussianNB\n",
    "1. The optimal var_smoothing parameter was found to be 0.01.\n",
    "2. The model achieved an overall accuracy of 71%, which is lower compared to the Multinomial Naive Bayes model.\n",
    "3. Class 0 (Negative Sentiment): </br>\n",
    "    a. Precision (0.73): Of all reviews predicted as negative, 73% were correctly classified. This reflects a moderate rate of false positives when predicting negative sentiment. </br>\n",
    "    b. Recall (0.62): The model successfully identified 62% of actual negative reviews, indicating a significant number of false negatives. </br>\n",
    "    c. F1-Score (0.67): The harmonic mean of precision and recall is lower for this class, suggesting room for improvement in correctly classifying negative sentiment.\n",
    "4. Class 1 (Positive Sentiment): </br>\n",
    "    a. Precision (0.69): The model correctly identified 69% of the reviews predicted as positive, indicating a relatively high rate of false positives. </br>\n",
    "    b. Recall (0.79): The model identified 79% of actual positive reviews, showing better performance in detecting positive sentiment compared to negative sentiment. </br>\n",
    "    c. F1-Score (0.74): Indicates a better balance between precision and recall for the positive class.\n",
    "5. The Gaussian Naive Bayes model, when used with TF-IDF feature extraction, shows a lower performance compared to the Multinomial Naive Bayes model. It performs better in detecting positive sentiment (higher recall) but struggles with identifying negative sentiment accurately. This suggests that Gaussian Naive Bayes, which assumes continuous features, might not be well-suited for discrete text data represented by TF-IDF vectors. Further optimization or a different model might yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
